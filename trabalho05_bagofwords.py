# -*- coding: utf-8 -*-
"""Trabalho05_BagOfWords.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13maVXJPu8TqkCRZXbOXYVOJMokEuN7lO

Aluna: Vitória Izabel Mendes Pinto

Sua tarefa será  gerar a matriz termo documento, dos documentos recuperados da internet e imprimir esta matriz na tela. Para tanto: 

a) Considere que todas as listas de sentenças devem ser transformadas em listas de vetores, onde cada item será uma das palavras da sentença. 

b) Todos  os  vetores  devem  ser  unidos  em  um  corpus  único  formando  uma  lista  de  vetores, onde cada item será um lexema.  

c) Este único corpus será usado para gerar o vocabulário. 

d) O  resultado  esperado  será  uma  matriz  termo  documento  criada  a partir  da  aplicação  da técnica bag of Words em todo o corpus.
"""

import requests
from bs4 import BeautifulSoup
import numpy as np
import pandas as pd

"""# Site 01

https://www.tableau.com/learn/articles/natural-language-processing-examples
"""

site01 = "https://www.tableau.com/learn/articles/natural-language-processing-examples"
html = requests.get(site01).text
bs4_site01 = BeautifulSoup(html, "html.parser")
 
get = bs4_site01.find_all('p')
text_dir1 = list(get)

text_array1 = []

for text in text_dir1:
    text_array1.append(text.get_text().split(" "))

# Tirando as repetições do site 01
palavras_site01 = []
for i in text_array1:
  for j in i:
    palavras_site01.append(j)

sem_repeticao1 = []
contador = 0

for palavra in palavras_site01:
    if palavra not in sem_repeticao1:
        sem_repeticao1.append(palavra)
        contador += palavras_site01.count(palavra)
        
    contador = 0

"""# Site 02

https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1
"""

site02 = "https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1"
html = requests.get(site02).text
bs4_site02 = BeautifulSoup(html, "html.parser")
 
get = bs4_site02.find_all('p')
text_dir2 = list(get)

text_array2 = []

for text in text_dir2:
    text_array2.append(text.get_text().split(" "))

# Tirando as repetições do site 02
palavras_site02 = []
for i in text_array2:
  for j in i:
    palavras_site02.append(j)

sem_repeticao2 = []
contador = 0

for palavra in palavras_site02:
    if palavra not in sem_repeticao2:
        sem_repeticao2.append(palavra)
        contador += palavras_site02.count(palavra)
    
    contador = 0

"""# Site 03
https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP

"""

site03 = "https://www.techtarget.com/searchenterpriseai/definition/natural-language-processing-NLP"
html = requests.get(site03).text
bs4_site03 = BeautifulSoup(html, "html.parser")
 
get = bs4_site03.find_all('p')
text_dir3 = list(get)

text_array3 = []

for text in text_dir3:
    text_array3.append(text.get_text().split(" "))

# Tirando as repetições do site 03
palavras_site03 = []
for i in text_array3:
  for j in i:
    palavras_site03.append(j)

sem_repeticao3 = []
contador = 0

for palavra in palavras_site03:
    if palavra not in sem_repeticao3:
        sem_repeticao3.append(palavra)
        contador += palavras_site03.count(palavra)
    
    contador = 0

"""# Site 04
https://hbr.org/2022/04/the-power-of-natural-language-processing

"""

site04 = "https://hbr.org/2022/04/the-power-of-natural-language-processing"
html = requests.get(site04).text
bs4_site04 = BeautifulSoup(html, "html.parser")
 
get = bs4_site04.find_all('p')
text_dir4 = list(get)

text_array4 = []

for text in text_dir4:
    text_array4.append(text.get_text().split(" "))

# Tirando as repetições do site 04
palavras_site04 = []
for i in text_array4:
  for j in i:
    palavras_site04.append(j)

sem_repeticao4 = []
contador = 0

for palavra in palavras_site04:
    if palavra not in sem_repeticao4:
        sem_repeticao4.append(palavra)
        contador += palavras_site04.count(palavra)
    
    contador = 0

"""# Site 05
https://www.thoughtworks.com/en-br/insights/decoder/n/natural-language-processing
"""

site05 = "https://www.thoughtworks.com/en-br/insights/decoder/n/natural-language-processing"
html = requests.get(site05).text
bs4_site05 = BeautifulSoup(html, "html.parser")
 
get = bs4_site05.find_all('p')
text_dir5 = list(get)

text_array5 = []

for text in text_dir5:
    text_array5.append(text.get_text().split(" "))

# Tirando as repetições do site 05
palavras_site05 = []
for i in text_array5:
  for j in i:
    palavras_site05.append(j)

sem_repeticao5 = []
contador = 0

for palavra in palavras_site05:
    if palavra not in sem_repeticao5:
        sem_repeticao5.append(palavra)
        contador += palavras_site05.count(palavra)
    
    contador = 0

"""União dos vetores

Contagem da Frequencia
"""

# Juntando os lexemas dos sites 01 a 05
todosLexemas = []
todosSites = [sem_repeticao1, sem_repeticao2, sem_repeticao3, sem_repeticao4, sem_repeticao5]

for site in todosSites:
  for palavra in site:

      if palavra not in todosLexemas:
          todosLexemas.append(palavra)

# Vendo a frequência de cada palavra em cada site e adicionando na lista
freq1 = []
freq2 = []
freq3 = []
freq4 = []
freq5 = []


contador = 0

for palavra in todosLexemas:
    contador += palavras_site01.count(palavra)
    freq1.append(contador)
    contador = 0

for palavra in todosLexemas:
    contador += palavras_site02.count(palavra)
    freq2.append(contador)
    contador = 0
  
for palavra in todosLexemas:
    contador += palavras_site03.count(palavra)
    freq3.append(contador)
    contador = 0

for palavra in todosLexemas:
    contador += palavras_site04.count(palavra)
    freq4.append(contador)
    contador = 0

for palavra in todosLexemas:
    contador += palavras_site05.count(palavra)
    freq5.append(contador)
    contador = 0

"""Impressão da matriz termo documento"""

# deixando o tamanho máximo de colunas o suficiente para caber todos os lexemas
from google.colab.data_table import DataTable
DataTable.max_columns = 2650  

# linhas da tabela, cada linha um documento
primeiraLinha = pd.Series({todosLexemas[i]: freq1[i] for i in range(len(todosLexemas))})
segundaLinha = pd.Series({todosLexemas[i]: freq2[i] for i in range(len(todosLexemas))})
terceiraLinha = pd.Series({todosLexemas[i]: freq3[i] for i in range(len(todosLexemas))})
quartaLinha = pd.Series({todosLexemas[i]: freq4[i] for i in range(len(todosLexemas))})
quintaLinha = pd.Series({todosLexemas[i]: freq5[i] for i in range(len(todosLexemas))})

df = pd.DataFrame([primeiraLinha, segundaLinha, terceiraLinha, quartaLinha, quintaLinha])
df.index = np.arange(1, len(df) + 1)
df.index.names = ['DOCUMENTOS']
df